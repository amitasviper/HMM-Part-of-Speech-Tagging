{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from math import log\n",
    "import numpy as np\n",
    "incrementer = 1.0\n",
    "\n",
    "\n",
    "def getFileContents(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFileFromCommandLine():\n",
    "    filename = sys.argv[1]\n",
    "    return getFileContents(filename)\n",
    "\n",
    "\n",
    "\n",
    "def splitWordTag(word_tag_pair):\n",
    "    splitted = word_tag_pair.split('/')\n",
    "    tag = splitted[-1]\n",
    "    word = '/'.join(splitted[:-1])\n",
    "    return word, tag\n",
    "\n",
    "\n",
    "\n",
    "def getUniqueTags(tagged_data):\n",
    "    tags = {}\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            if tag in tags.keys():\n",
    "                tags[tag] += 1\n",
    "            else:\n",
    "                tags[tag] = 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "def getUniqueWords(tagged_data):\n",
    "    words = []\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            words.append(word)\n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "def readModelFile():\n",
    "    filename = 'hmmmodel.txt'\n",
    "    lines = []\n",
    "    with open(filename, 'r') as model_file:\n",
    "        lines = model_file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def parseModel(lines):\n",
    "    total_tags = int(lines[0].strip().split(':')[-1])\n",
    "    total_words = int(lines[1].strip().split(':')[-1])\n",
    "    \n",
    "    tr_start_line_number = int(lines[2].strip().split(':')[-2])\n",
    "    tr_end_line_number = int(lines[2].strip().split(':')[-1])\n",
    "    \n",
    "    em_start_line_number = int(lines[3].strip().split(':')[-2])\n",
    "    em_end_line_number = int(lines[3].strip().split(':')[-1])\n",
    "    \n",
    "    oc_start_line_number = int(lines[4].strip().split(':')[-2])\n",
    "    oc_end_line_number = int(lines[4].strip().split(':')[-1])\n",
    "    \n",
    "    wi_start_line_number = int(lines[5].strip().split(':')[-2])\n",
    "    wi_end_line_number = int(lines[5].strip().split(':')[-1])\n",
    "    \n",
    "    af_start_line_number = int(lines[6].strip().split(':')[-2])\n",
    "    af_end_line_number = int(lines[6].strip().split(':')[-1])\n",
    "    \n",
    "#     print total_tags, total_words, tr_start_line_number, tr_end_line_number, em_start_line_number, em_end_line_number, oc_start_line_number,oc_end_line_number, wi_start_line_number, wi_end_line_number\n",
    "    \n",
    "    probability_transition_matrix = []\n",
    "    for line_number in range(tr_start_line_number, tr_end_line_number, 1):\n",
    "        row_values = map(float, lines[line_number].strip().split('\\t'))\n",
    "        probability_transition_matrix.append(row_values)\n",
    "    \n",
    "    probability_emission_matrix = []\n",
    "    for line_number in range(em_start_line_number, em_end_line_number, 1):\n",
    "        row_values = map(float, lines[line_number].strip().split('\\t'))\n",
    "        probability_emission_matrix.append(row_values)\n",
    "        \n",
    "    \n",
    "    opening_probabilities = {}\n",
    "    closing_probabilities = {}\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    tags_index_dict_reverse = {}\n",
    "    \n",
    "    for line_number in range(oc_start_line_number, oc_end_line_number, 1):\n",
    "        row_values = lines[line_number].strip().split('\\t')\n",
    "        tag_name = row_values[0]\n",
    "        open_p = float(row_values[1])\n",
    "        close_p = float(row_values[2])\n",
    "        index = int(row_values[3])\n",
    "        \n",
    "        opening_probabilities[tag_name] = open_p\n",
    "        closing_probabilities[tag_name] = close_p\n",
    "        tags_index_dict[tag_name] = index\n",
    "        tags_index_dict_reverse[index] = tag_name\n",
    "    \n",
    "    words_index_dict = {}\n",
    "    words_index_dict_reverse = {}\n",
    "    \n",
    "    for line_number in range(wi_start_line_number, wi_end_line_number, 1):\n",
    "        row_values = lines[line_number].strip().split('\\t')\n",
    "        word = row_values[0]\n",
    "        index = int(row_values[1])\n",
    "        words_index_dict[word] = index\n",
    "        words_index_dict_reverse[index] = word\n",
    "        \n",
    "    additional_features = {}\n",
    "    for line_number in range(af_start_line_number, af_end_line_number, 1):\n",
    "        row_values = lines[line_number].strip().split('\\t')\n",
    "        feature_name = row_values[0]\n",
    "        feature_tag = row_values[1]\n",
    "        additional_features[feature_name] = feature_tag\n",
    "    \n",
    "        \n",
    "    return opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse, additional_features\n",
    "\n",
    "\n",
    "\n",
    "def getMostProbableTags(sentence):\n",
    "    global opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse \n",
    "    global tag_count, unseen_words, additional_features\n",
    "    \n",
    "    sentence_words = sentence.strip().split(' ')\n",
    "    \n",
    "    sentence_len = len(sentence_words)\n",
    "    \n",
    "    viterbi_matrix = np.zeros(shape=(tag_count, sentence_len))\n",
    "    \n",
    "    tracing_matrix = [[None for x in range(sentence_len)] for y in range(tag_count)]\n",
    "    \n",
    "    for col in range(sentence_len):\n",
    "        word = sentence_words[col]\n",
    "        for model_tag in tags_index_dict:\n",
    "            model_tag_index = tags_index_dict[model_tag]\n",
    "            try:\n",
    "                word_emission_probability = probability_emission_matrix[model_tag_index][words_index_dict[word]]\n",
    "                if word_emission_probability == 0.0:\n",
    "                    continue\n",
    "            except KeyError as e:\n",
    "                if word.count('=') > 10 or word.count('_') > 10 or word.count('*') > 10 or word.count('-') > 10 or word.count('+') > 10:\n",
    "                    if model_tag == additional_features['PAGE_SEP']:\n",
    "                        word_emission_probability = 1.0\n",
    "                    else:\n",
    "                        word_emission_probability = 1.1754943508222875e-30\n",
    "                elif any(word.lower().endswith(last) for last in ('.com', '.net', '.org', '.edu')) or word.startswith('http') or word.startswith('www.'):\n",
    "                    if model_tag == additional_features['URLS']:\n",
    "                        word_emission_probability = 1.0\n",
    "                    else:\n",
    "                        word_emission_probability = 1.1754943508222875e-30\n",
    "                elif [char.isdigit() for char in word].count(True) * 1.0 > len(word) * 0.4:\n",
    "                    if model_tag == additional_features['NUMERICS']:\n",
    "                        word_emission_probability = 1.0\n",
    "                    else:\n",
    "                        word_emission_probability = 1.1754943508222875e-30\n",
    "                else:\n",
    "                    try:\n",
    "                        word_emission_probability = probability_emission_matrix[model_tag_index][words_index_dict[word.lower()]]\n",
    "                    except KeyError as e:\n",
    "                        word_emission_probability = 1.0 #probability_emission_matrix[model_tag_index][-1]\n",
    "            \n",
    "            if col == 0:\n",
    "                try:\n",
    "                    tag_opening_probability = opening_probabilities[model_tag]\n",
    "                except KeyError as e:\n",
    "                    print \"tag_opening_probability : Keyerror encountered\"\n",
    "                    tag_opening_probability = 1.1754943508222875e-100\n",
    "                viterbi_matrix[model_tag_index][col] = tag_opening_probability * word_emission_probability\n",
    "            else:\n",
    "                max_probability = 0.0 #np.finfo(float).min\n",
    "                max_tag = None\n",
    "                \n",
    "                for prev_model_tag in tags_index_dict:\n",
    "                    prev_model_tag_index = tags_index_dict[prev_model_tag]\n",
    "                    tag_transition_probability = probability_transition_matrix[prev_model_tag_index][model_tag_index]\n",
    "                    if tag_transition_probability == 0.0:\n",
    "                        print \"Transition probability still zero\"\n",
    "                        tag_transition_probability = 1.1754943508222875e-100\n",
    "                    temp_probability = viterbi_matrix[prev_model_tag_index][col-1] * tag_transition_probability * word_emission_probability  \n",
    "                    if temp_probability >= max_probability:\n",
    "                        max_probability = temp_probability\n",
    "                        max_tag = prev_model_tag\n",
    "                        \n",
    "                viterbi_matrix[model_tag_index][col] = max_probability\n",
    "                tracing_matrix[model_tag_index][col] = max_tag\n",
    "    \n",
    "    max_probability = 0.0 #np.finfo(float).min\n",
    "    max_probability_tag = None\n",
    "    for model_tag in tags_index_dict:\n",
    "        model_tag_index = tags_index_dict[model_tag]\n",
    "        temp_probability = 0.0\n",
    "        try:\n",
    "            tag_closing_probabilities = closing_probabilities[model_tag]\n",
    "        except KeyError as e:\n",
    "            print \"tag_closing_probabilities : Keyerror encountered\", \n",
    "            tag_closing_probabilities = 1.1754943508222875e-100\n",
    "        temp_probability =  tag_closing_probabilities * viterbi_matrix[model_tag_index][sentence_len-1]\n",
    "        if temp_probability >= max_probability:\n",
    "            max_probability = temp_probability\n",
    "            max_probability_tag = model_tag\n",
    "\n",
    "    assigned_tags = [max_probability_tag]\n",
    "    current_best_tag = max_probability_tag\n",
    "    \n",
    "    for col in range(sentence_len-1, 0, -1):\n",
    "        current_best_tag = tracing_matrix[tags_index_dict[current_best_tag]][col]\n",
    "        assigned_tags.append(current_best_tag)\n",
    "    assigned_tags = assigned_tags[::-1]\n",
    "    \n",
    "    anotated_sentence = ''\n",
    "    for index, assigned_tag in enumerate(assigned_tags):\n",
    "        anotated_sentence += str(sentence_words[index]) + '/' + str(assigned_tag) + ' '\n",
    "    \n",
    "    \n",
    "    return anotated_sentence.strip()\n",
    "\n",
    "def startPredicting():\n",
    "    test_data = getFileFromCommandLine()\n",
    "#     test_data = getFileContents('data/zh_dev_raw.txt')\n",
    "    output = ''\n",
    "    for test_line in test_data:\n",
    "        predicted_tagged_line = getMostProbableTags(test_line)\n",
    "        output += predicted_tagged_line + '\\n'\n",
    "    \n",
    "    with open('hmmoutput.txt', 'w') as output_file:\n",
    "        output_file.write(output)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lines = readModelFile()\n",
    "    opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse, additional_features  = parseModel(lines)\n",
    "    print additional_features\n",
    "    tag_count = len(tags_index_dict.keys())\n",
    "    startPredicting()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getFileContents(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "def computeAccuracy():\n",
    "    dev_tagged_data = getFileContents('data/zh_dev_tagged.txt')\n",
    "    predicted_data = getFileContents('hmmoutput.txt')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index, line in enumerate(dev_tagged_data):\n",
    "        predicted_tagged_line = predicted_data[index]\n",
    "        expected_tagged_line = dev_tagged_data[index]\n",
    "        \n",
    "        predicted_word_tag_pairs = predicted_tagged_line.strip().split(' ')\n",
    "        expected_word_tag_pairs = expected_tagged_line.strip().split(' ')\n",
    "        for index, predicted_word in enumerate(predicted_word_tag_pairs):\n",
    "            if predicted_word == expected_word_tag_pairs[index]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print predicted_word.ljust(30), ' => ',expected_word_tag_pairs[index]\n",
    "            total += 1\n",
    "#             if total % 100 == 0:\n",
    "#                 print correct, total, \" => \", (correct*100.0)/total\n",
    "    accuracy = (correct*100.0)/total\n",
    "    print accuracy\n",
    "\n",
    "computeAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese => 86.306562426 => 86.306562426\n",
    "# English => 88.9891840305 => 89.0090663273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def getAllRepos():\n",
    "    tokens = '&client_id=1124b4c881831a34f303&client_secret=a6f507d2114ff15fcec03839efa884bf5aa8a4ad'\n",
    "    repo_url = 'https://api.github.com/users/UniversalDependencies/repos?per_page=1000&type=all&page=1'+tokens\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    resp = requests.get(repo_url, headers=headers)\n",
    "    repo_names = []\n",
    "    for repo in resp.json():\n",
    "        if repo['name'] and repo['name'].startswith('UD'):\n",
    "            repo_names.append(repo['name'])\n",
    "    return repo_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names = getAllRepos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'UD_Afrikaans',\n",
       " u'UD_Amharic-ATT',\n",
       " u'UD_Ancient_Greek',\n",
       " u'UD_Ancient_Greek-PROIEL',\n",
       " u'UD_Arabic-NYUAD']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "import urlparse\n",
    "def getFilesList(user_name, repo_name):\n",
    "    tokens = '?client_id=1124b4c881831a34f303&client_secret=a6f507d2114ff15fcec03839efa884bf5aa8a4ad'\n",
    "    repo_url = 'https://api.github.com/repos/' +  user_name + '/' + repo_name + '/contents' + tokens\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    resp = requests.get(repo_url, headers=headers)\n",
    "    valid_urls = []\n",
    "    for file_info in resp.json():\n",
    "        if file_info['download_url'] and file_info['download_url'].endswith('.conllu'):\n",
    "            valid_urls.append(file_info['download_url']) \n",
    "    return valid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for repo in repo_names:\n",
    "    urls = getFilesList('UniversalDependencies', repo)\n",
    "    mapping[repo] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'https://raw.githubusercontent.com/UniversalDependencies/UD_Marathi/master/mr-ud-dev.conllu',\n",
       " u'https://raw.githubusercontent.com/UniversalDependencies/UD_Marathi/master/mr-ud-test.conllu',\n",
       " u'https://raw.githubusercontent.com/UniversalDependencies/UD_Marathi/master/mr-ud-train.conllu']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['UD_Marathi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urlparse, os\n",
    "\n",
    "def fetchPage(url):\n",
    "    data = requests.get(url)\n",
    "    return data.text.split('\\n')\n",
    "\n",
    "def parseSentences(data):\n",
    "    tagged_data = []\n",
    "    raw_sentence = []\n",
    "    flag_new_sentence = True\n",
    "    word_tags = []\n",
    "    words = []\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        if line.startswith('#'):\n",
    "            flag_new_sentence = False\n",
    "            continue\n",
    "        if line == '':\n",
    "            flag_new_sentence = True\n",
    "            tagged_data.append(' '.join(word_tags))\n",
    "            raw_sentence.append(' '.join(words))\n",
    "            words = []\n",
    "            word_tags = []\n",
    "            continue\n",
    "        res = line.split('\\t')\n",
    "        \n",
    "        words.append(res[1])\n",
    "        word_tags.append(res[1] + '/' + res[4])\n",
    "    return tagged_data, raw_sentence\n",
    "\n",
    "def writeToFile(url, tagged_data, raw_sentence):\n",
    "    a = urlparse.urlparse(url)\n",
    "    filename = os.path.basename(a.path)\n",
    "    \n",
    "    with open('raw_'+filename, 'w') as output_file:\n",
    "        output_file.write('\\n'.join(raw_sentence).encode('utf_8'))\n",
    "        \n",
    "    with open(filename, 'w') as output_file:\n",
    "        output_file.write('\\n'.join(tagged_data).encode('utf_8'))\n",
    "        \n",
    "# url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Bulgarian-BTB/master/bg-ud-test.conllu'\n",
    "# data = fetchPage(url)\n",
    "# tagged_data, raw_sentence = parseSentences(data)\n",
    "# writeToFile(url, tagged_data, raw_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileName(url):\n",
    "    a = urlparse.urlparse(url)\n",
    "    filename = os.path.basename(a.path)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def train_N_test():\n",
    "    for language in mapping.keys():\n",
    "        urls = mapping[language]\n",
    "        test_url = ''\n",
    "        train_url = ''\n",
    "        for url in urls:\n",
    "            if url.endswith('train.conllu'):\n",
    "                train_url = url\n",
    "            elif url.endswith('test.conllu'):\n",
    "                test_url = url\n",
    "            else:\n",
    "                pass\n",
    "        if test_url == '' or train_url == '':\n",
    "            continue\n",
    "        \n",
    "        data = fetchPage(test_url)\n",
    "        tagged_data, raw_sentence = parseSentences(data)\n",
    "        writeToFile(test_url, tagged_data, raw_sentence)\n",
    "        \n",
    "        \n",
    "        data = fetchPage(train_url)\n",
    "        tagged_data, raw_sentence = parseSentences(data)\n",
    "        writeToFile(train_url, tagged_data, raw_sentence)\n",
    "        \n",
    "        train_tagged_file = getFileName(train_url)\n",
    "        test_raw_file = 'raw_' + getFileName(test_url)\n",
    "        test_tagged_file = getFileName(test_url)\n",
    "        \n",
    "        train_model_command = ['python','hmmlearn.py', train_tagged_file]\n",
    "        \n",
    "        test_model_command = ['python','hmmdecode.py', test_raw_file]\n",
    "        \n",
    "        accuracy_check_command = ['python', 'accuracyTest.py', test_tagged_file]\n",
    "        \n",
    "#         result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n",
    "#         result.stdout.decode('utf-8')\n",
    "        \n",
    "        print ' '.join(train_model_command)\n",
    "        \n",
    "#         process = subprocess.Popen(train_model_command, stdout=subprocess.PIPE)\n",
    "#         process.wait()\n",
    "#         stdout = process.communicate()[0]\n",
    "#         print stdout\n",
    "        \n",
    "        print ' '.join(test_model_command)\n",
    "        \n",
    "#         process = subprocess.Popen(test_model_command, stdout=subprocess.PIPE)\n",
    "#         process.wait()\n",
    "#         stdout = process.communicate()[0]\n",
    "#         print stdout\n",
    "        \n",
    "        print ' '.join(accuracy_check_command)\n",
    "        \n",
    "#         process = subprocess.Popen(accuracy_check_command, stdout=subprocess.PIPE)\n",
    "#         process.wait()\n",
    "#         stdout = process.communicate()[0]\n",
    "#         print stdout\n",
    "        \n",
    "#         while True:\n",
    "#             out = process.stdout.read(1)\n",
    "#             if out == '' and process.poll() != None:\n",
    "#                 break\n",
    "#             if out != '':\n",
    "#                 sys.stdout.write(out)\n",
    "#                 sys.stdout.flush()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python hmmlearn.py mr-ud-train.conllu\n",
      "python hmmdecode.py raw_mr-ud-test.conllu\n",
      "python accuracyTest.py mr-ud-test.conllu\n",
      "python hmmlearn.py no_bokmaal-ud-train.conllu\n",
      "python hmmdecode.py raw_no_bokmaal-ud-test.conllu\n",
      "python accuracyTest.py no_bokmaal-ud-test.conllu\n",
      "python hmmlearn.py it_partut-ud-train.conllu\n",
      "python hmmdecode.py raw_it_partut-ud-test.conllu\n",
      "python accuracyTest.py it_partut-ud-test.conllu\n",
      "python hmmlearn.py la_ittb-ud-train.conllu\n",
      "python hmmdecode.py raw_la_ittb-ud-test.conllu\n",
      "python accuracyTest.py la_ittb-ud-test.conllu\n",
      "python hmmlearn.py da-ud-train.conllu\n",
      "python hmmdecode.py raw_da-ud-test.conllu\n",
      "python accuracyTest.py da-ud-test.conllu\n",
      "python hmmlearn.py hu-ud-train.conllu\n",
      "python hmmdecode.py raw_hu-ud-test.conllu\n",
      "python accuracyTest.py hu-ud-test.conllu\n",
      "python hmmlearn.py hi-ud-train.conllu\n",
      "python hmmdecode.py raw_hi-ud-test.conllu\n",
      "python accuracyTest.py hi-ud-test.conllu\n",
      "python hmmlearn.py grc_proiel-ud-train.conllu\n",
      "python hmmdecode.py raw_grc_proiel-ud-test.conllu\n",
      "python accuracyTest.py grc_proiel-ud-test.conllu\n",
      "python hmmlearn.py en-ud-train.conllu\n",
      "python hmmdecode.py raw_en-ud-test.conllu\n",
      "python accuracyTest.py en-ud-test.conllu\n",
      "python hmmlearn.py fr_partut-ud-train.conllu\n",
      "python hmmdecode.py raw_fr_partut-ud-test.conllu\n",
      "python accuracyTest.py fr_partut-ud-test.conllu\n",
      "python hmmlearn.py ar-ud-train.conllu\n",
      "python hmmdecode.py raw_ar-ud-test.conllu\n",
      "python accuracyTest.py ar-ud-test.conllu\n",
      "python hmmlearn.py lt-ud-train.conllu\n",
      "python hmmdecode.py raw_lt-ud-test.conllu\n",
      "python accuracyTest.py lt-ud-test.conllu\n",
      "python hmmlearn.py el-ud-train.conllu\n",
      "python hmmdecode.py raw_el-ud-test.conllu\n",
      "python accuracyTest.py el-ud-test.conllu\n",
      "python hmmlearn.py no_nynorsk-ud-train.conllu\n",
      "python hmmdecode.py raw_no_nynorsk-ud-test.conllu\n",
      "python accuracyTest.py no_nynorsk-ud-test.conllu\n",
      "python hmmlearn.py lv-ud-train.conllu\n",
      "python hmmdecode.py raw_lv-ud-test.conllu\n",
      "python accuracyTest.py lv-ud-test.conllu\n",
      "python hmmlearn.py ga-ud-train.conllu\n",
      "python hmmdecode.py raw_ga-ud-test.conllu\n",
      "python accuracyTest.py ga-ud-test.conllu\n",
      "python hmmlearn.py gl-ud-train.conllu\n",
      "python hmmdecode.py raw_gl-ud-test.conllu\n",
      "python accuracyTest.py gl-ud-test.conllu\n",
      "python hmmlearn.py gl_treegal-ud-train.conllu\n",
      "python hmmdecode.py raw_gl_treegal-ud-test.conllu\n",
      "python accuracyTest.py gl_treegal-ud-test.conllu\n",
      "python hmmlearn.py fr-ud-train.conllu\n",
      "python hmmdecode.py raw_fr-ud-test.conllu\n",
      "python accuracyTest.py fr-ud-test.conllu\n",
      "python hmmlearn.py zh-ud-train.conllu\n",
      "python hmmdecode.py raw_zh-ud-test.conllu\n",
      "python accuracyTest.py zh-ud-test.conllu\n",
      "python hmmlearn.py ca_ancora-ud-train.conllu\n",
      "python hmmdecode.py raw_ca_ancora-ud-test.conllu\n",
      "python accuracyTest.py ca_ancora-ud-test.conllu\n",
      "python hmmlearn.py cop-ud-train.conllu\n",
      "python hmmdecode.py raw_cop-ud-test.conllu\n",
      "python accuracyTest.py cop-ud-test.conllu\n",
      "python hmmlearn.py fi_ftb-ud-train.conllu\n",
      "python hmmdecode.py raw_fi_ftb-ud-test.conllu\n",
      "python accuracyTest.py fi_ftb-ud-test.conllu\n",
      "python hmmlearn.py id-ud-train.conllu\n",
      "python hmmdecode.py raw_id-ud-test.conllu\n",
      "python accuracyTest.py id-ud-test.conllu\n",
      "python hmmlearn.py en_partut-ud-train.conllu\n",
      "python hmmdecode.py raw_en_partut-ud-test.conllu\n",
      "python accuracyTest.py en_partut-ud-test.conllu\n",
      "python hmmlearn.py nl-ud-train.conllu\n",
      "python hmmdecode.py raw_nl-ud-test.conllu\n",
      "python accuracyTest.py nl-ud-test.conllu\n",
      "python hmmlearn.py fi-ud-train.conllu\n",
      "python hmmdecode.py raw_fi-ud-test.conllu\n",
      "python accuracyTest.py fi-ud-test.conllu\n",
      "python hmmlearn.py et-ud-train.conllu\n",
      "python hmmdecode.py raw_et-ud-test.conllu\n",
      "python accuracyTest.py et-ud-test.conllu\n",
      "python hmmlearn.py hr-ud-train.conllu\n",
      "python hmmdecode.py raw_hr-ud-test.conllu\n",
      "python accuracyTest.py hr-ud-test.conllu\n",
      "python hmmlearn.py la_proiel-ud-train.conllu\n",
      "python hmmdecode.py raw_la_proiel-ud-test.conllu\n",
      "python accuracyTest.py la_proiel-ud-test.conllu\n",
      "python hmmlearn.py fr_ftb-ud-train.conllu\n",
      "python hmmdecode.py raw_fr_ftb-ud-test.conllu\n",
      "python accuracyTest.py fr_ftb-ud-test.conllu\n",
      "python hmmlearn.py de-ud-train.conllu\n",
      "python hmmdecode.py raw_de-ud-test.conllu\n",
      "python accuracyTest.py de-ud-test.conllu\n",
      "python hmmlearn.py he-ud-train.conllu\n",
      "python hmmdecode.py raw_he-ud-test.conllu\n",
      "python accuracyTest.py he-ud-test.conllu\n",
      "python hmmlearn.py cs_cltt-ud-train.conllu\n",
      "python hmmdecode.py raw_cs_cltt-ud-test.conllu\n",
      "python accuracyTest.py cs_cltt-ud-test.conllu\n",
      "python hmmlearn.py it_postwita-ud-train.conllu\n",
      "python hmmdecode.py raw_it_postwita-ud-test.conllu\n",
      "python accuracyTest.py it_postwita-ud-test.conllu\n",
      "python hmmlearn.py fr_sequoia-ud-train.conllu\n",
      "python hmmdecode.py raw_fr_sequoia-ud-test.conllu\n",
      "python accuracyTest.py fr_sequoia-ud-test.conllu\n",
      "python hmmlearn.py got-ud-train.conllu\n",
      "python hmmdecode.py raw_got-ud-test.conllu\n",
      "python accuracyTest.py got-ud-test.conllu\n",
      "python hmmlearn.py it-ud-train.conllu\n",
      "python hmmdecode.py raw_it-ud-test.conllu\n",
      "python accuracyTest.py it-ud-test.conllu\n",
      "python hmmlearn.py ja-ud-train.conllu\n",
      "python hmmdecode.py raw_ja-ud-test.conllu\n",
      "python accuracyTest.py ja-ud-test.conllu\n",
      "python hmmlearn.py cs_fictree-ud-train.conllu\n",
      "python hmmdecode.py raw_cs_fictree-ud-test.conllu\n",
      "python accuracyTest.py cs_fictree-ud-test.conllu\n",
      "python hmmlearn.py sme-ud-train.conllu\n",
      "python hmmdecode.py raw_sme-ud-test.conllu\n",
      "python accuracyTest.py sme-ud-test.conllu\n",
      "python hmmlearn.py la-ud-train.conllu\n",
      "python hmmdecode.py raw_la-ud-test.conllu\n",
      "python accuracyTest.py la-ud-test.conllu\n",
      "python hmmlearn.py nl_lassysmall-ud-train.conllu\n",
      "python hmmdecode.py raw_nl_lassysmall-ud-test.conllu\n",
      "python accuracyTest.py nl_lassysmall-ud-test.conllu\n",
      "python hmmlearn.py bg-ud-train.conllu\n",
      "python hmmdecode.py raw_bg-ud-test.conllu\n",
      "python accuracyTest.py bg-ud-test.conllu\n",
      "python hmmlearn.py cs_cac-ud-train.conllu\n",
      "python hmmdecode.py raw_cs_cac-ud-test.conllu\n",
      "python accuracyTest.py cs_cac-ud-test.conllu\n",
      "python hmmlearn.py ko-ud-train.conllu\n",
      "python hmmdecode.py raw_ko-ud-test.conllu\n",
      "python accuracyTest.py ko-ud-test.conllu\n",
      "python hmmlearn.py ar_nyuad-ud-train.conllu\n",
      "python hmmdecode.py raw_ar_nyuad-ud-test.conllu\n",
      "python accuracyTest.py ar_nyuad-ud-test.conllu\n",
      "python hmmlearn.py grc-ud-train.conllu\n",
      "python hmmdecode.py raw_grc-ud-test.conllu\n",
      "python accuracyTest.py grc-ud-test.conllu\n",
      "python hmmlearn.py en_lines-ud-train.conllu\n",
      "python hmmdecode.py raw_en_lines-ud-test.conllu\n",
      "python accuracyTest.py en_lines-ud-test.conllu\n",
      "python hmmlearn.py be-ud-train.conllu\n",
      "python hmmdecode.py raw_be-ud-test.conllu\n",
      "python accuracyTest.py be-ud-test.conllu\n",
      "python hmmlearn.py eu-ud-train.conllu\n",
      "python hmmdecode.py raw_eu-ud-test.conllu\n",
      "python accuracyTest.py eu-ud-test.conllu\n",
      "python hmmlearn.py af-ud-train.conllu\n",
      "python hmmdecode.py raw_af-ud-test.conllu\n",
      "python accuracyTest.py af-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "train_N_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
