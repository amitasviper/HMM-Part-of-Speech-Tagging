{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 17610 7 49 50 92 93 135 136 17746\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from math import log\n",
    "import numpy as np\n",
    "incrementer = 0.00000000000000000000000000001\n",
    "\n",
    "\n",
    "def getFileContents(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFileFromCommandLine():\n",
    "    filename = sys.argv[1]\n",
    "    return getFileContents(filename)\n",
    "\n",
    "\n",
    "\n",
    "def splitWordTag(word_tag_pair):\n",
    "    splitted = word_tag_pair.split('/')\n",
    "    tag = splitted[-1]\n",
    "    word = '/'.join(splitted[:-1])\n",
    "    return word, tag\n",
    "\n",
    "\n",
    "\n",
    "def getUniqueTags(tagged_data):\n",
    "    tags = {}\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            if tag in tags.keys():\n",
    "                tags[tag] += 1\n",
    "            else:\n",
    "                tags[tag] = 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "def getUniqueWords(tagged_data):\n",
    "    words = []\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            words.append(word)\n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "def readModelFile():\n",
    "    filename = 'hmmmodel.txt'\n",
    "    lines = []\n",
    "    with open(filename, 'r') as model_file:\n",
    "        lines = model_file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def parseModel(lines):\n",
    "    total_tags = int(lines[0].strip().split(':')[-1])\n",
    "    total_words = int(lines[1].strip().split(':')[-1])\n",
    "    \n",
    "    tr_start_line_number = int(lines[2].strip().split(':')[-2])\n",
    "    tr_end_line_number = int(lines[2].strip().split(':')[-1])\n",
    "    \n",
    "    em_start_line_number = int(lines[3].strip().split(':')[-2])\n",
    "    em_end_line_number = int(lines[3].strip().split(':')[-1])\n",
    "    \n",
    "    oc_start_line_number = int(lines[4].strip().split(':')[-2])\n",
    "    oc_end_line_number = int(lines[4].strip().split(':')[-1])\n",
    "    \n",
    "    wi_start_line_number = int(lines[5].strip().split(':')[-2])\n",
    "    wi_end_line_number = int(lines[5].strip().split(':')[-1])\n",
    "    \n",
    "    print total_tags, total_words, tr_start_line_number, tr_end_line_number, em_start_line_number, em_end_line_number, oc_start_line_number,oc_end_line_number, wi_start_line_number, wi_end_line_number\n",
    "    \n",
    "    probability_transition_matrix = []\n",
    "    for line_number in range(tr_start_line_number, tr_end_line_number, 1):\n",
    "        row_values = map(float, lines[line_number].strip().split('\\t'))\n",
    "        probability_transition_matrix.append(row_values)\n",
    "    \n",
    "    probability_emission_matrix = []\n",
    "    for line_number in range(em_start_line_number, em_end_line_number, 1):\n",
    "        row_values = map(float, lines[line_number].strip().split('\\t'))\n",
    "        probability_emission_matrix.append(row_values)\n",
    "        \n",
    "    \n",
    "    opening_probabilities = {}\n",
    "    closing_probabilities = {}\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    tags_index_dict_reverse = {}\n",
    "    \n",
    "    for line_number in range(oc_start_line_number, oc_end_line_number, 1):\n",
    "        row_values = lines[line_number].strip().split('\\t')\n",
    "        tag_name = row_values[0]\n",
    "        open_p = float(row_values[1])\n",
    "        close_p = float(row_values[2])\n",
    "        index = int(row_values[3])\n",
    "        \n",
    "        opening_probabilities[tag_name] = open_p\n",
    "        closing_probabilities[tag_name] = close_p\n",
    "        tags_index_dict[tag_name] = index\n",
    "        tags_index_dict_reverse[index] = tag_name\n",
    "    \n",
    "    words_index_dict = {}\n",
    "    words_index_dict_reverse = {}\n",
    "    \n",
    "    for line_number in range(wi_start_line_number, wi_end_line_number, 1):\n",
    "        row_values = lines[line_number].strip().split('\\t')\n",
    "        word = row_values[0]\n",
    "        index = int(row_values[1])\n",
    "        words_index_dict[word] = index\n",
    "        words_index_dict_reverse[index] = word\n",
    "        \n",
    "    return opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse \n",
    "\n",
    "def getMostProbableTags(sentence):\n",
    "    global opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse \n",
    "    global tag_count, unseen_words\n",
    "    \n",
    "    sentence_words = sentence.strip().split(' ')\n",
    "    \n",
    "    sentence_len = len(sentence_words)\n",
    "    \n",
    "    viterbi_matrix = np.zeros(shape=(tag_count, sentence_len))\n",
    "    \n",
    "    tracing_matrix = [[None for x in range(sentence_len)] for y in range(tag_count)]\n",
    "    \n",
    "    for word_index in range(sentence_len):\n",
    "        word = sentence_words[word_index]\n",
    "        for model_tag in tags_index_dict:\n",
    "            model_tag_index = tags_index_dict[model_tag]\n",
    "            try:\n",
    "                word_emission_probability = probability_emission_matrix[model_tag_index][words_index_dict[word]]\n",
    "            except KeyError as e:\n",
    "                word_emission_probability = 1.0  #probability_emission_matrix[model_tag_index][-1]\n",
    "            \n",
    "            if word_index == 0:\n",
    "                try:\n",
    "                    tag_opening_probability = opening_probabilities[model_tag]\n",
    "                except KeyError as e:\n",
    "                    print \"tag_opening_probability : Keyerror encountered\"\n",
    "                    tag_opening_probability = 1.1754943508222875e-10\n",
    "                viterbi_matrix[model_tag_index][word_index] = tag_opening_probability + word_emission_probability\n",
    "            else:\n",
    "                max_probability = np.finfo(float).min\n",
    "                max_tag = None\n",
    "                for prev_model_tag in tags_index_dict:\n",
    "                    prev_model_tag_index = tags_index_dict[prev_model_tag]\n",
    "                    tag_transition_probability = probability_transition_matrix[prev_model_tag_index][model_tag_index]\n",
    "#                     if tag_transition_probability == 0.0:\n",
    "#                         print \"Transition probability still zero\"\n",
    "#                         tag_transition_probability = 1.1754943508222875e-10\n",
    "                    temp_probability = viterbi_matrix[prev_model_tag_index][word_index-1] + tag_transition_probability + word_emission_probability  \n",
    "                    if temp_probability > max_probability:\n",
    "                        max_probability = temp_probability\n",
    "                        max_tag = prev_model_tag\n",
    "                        \n",
    "                viterbi_matrix[model_tag_index][word_index] = max_probability\n",
    "                tracing_matrix[model_tag_index][word_index] = max_tag\n",
    "    \n",
    "    max_probability = np.finfo(float).min\n",
    "    max_probability_tag = None\n",
    "    for model_tag in tags_index_dict:\n",
    "        model_tag_index = tags_index_dict[model_tag]\n",
    "        temp_probability = 0.0\n",
    "        try:\n",
    "            tag_closing_probabilities = closing_probabilities[model_tag]\n",
    "        except KeyError as e:\n",
    "            print \"tag_closing_probabilities : Keyerror encountered\", \n",
    "            tag_closing_probabilities = 1.1754943508222875e-10\n",
    "        temp_probability =  tag_closing_probabilities + viterbi_matrix[model_tag_index][sentence_len-1]\n",
    "        if temp_probability > max_probability:\n",
    "            max_probability = temp_probability\n",
    "            max_probability_tag = model_tag\n",
    "\n",
    "    assigned_tags = [max_probability_tag]\n",
    "    current_best_tag = max_probability_tag\n",
    "    for col in range(sentence_len-1, 0, -1):\n",
    "        current_best_tag = tracing_matrix[tags_index_dict[current_best_tag]][col]\n",
    "        assigned_tags.append(current_best_tag)\n",
    "    assigned_tags = assigned_tags[::-1]\n",
    "    \n",
    "    anotated_sentence = ''\n",
    "    for index, assigned_tag in enumerate(assigned_tags):\n",
    "        anotated_sentence += str(sentence_words[index]) + '/' + str(assigned_tag) + ' '\n",
    "    \n",
    "    \n",
    "    return anotated_sentence.strip()\n",
    "\n",
    "def startPredicting():\n",
    "#     test_data = getFileFromCommandLine()\n",
    "    test_data = getFileContents('data/zh_dev_raw.txt')\n",
    "    output = ''\n",
    "    for test_line in test_data:\n",
    "        predicted_tagged_line = getMostProbableTags(test_line)\n",
    "        output += predicted_tagged_line + '\\n'\n",
    "    \n",
    "    with open('hmmoutput.txt', 'w') as output_file:\n",
    "        output_file.write(output)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lines = readModelFile()\n",
    "    opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, tags_index_dict_reverse, words_index_dict, words_index_dict_reverse  = parseModel(lines)\n",
    "    tag_count = len(tags_index_dict.keys())\n",
    "    startPredicting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
