{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileContents(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileFromCommandLine():\n",
    "    filename = sys.argv[1]\n",
    "    return getFileContents(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWordTag(word_tag_pair):\n",
    "    splitted = word_tag_pair.split('/')\n",
    "    tag = splitted[-1]\n",
    "    word = '/'.join(splitted[:-1])\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueTags(tagged_data):\n",
    "    tags = {}\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "#             if len(splitted) > 2:\n",
    "#                 print splitted, tag, word\n",
    "#                 print line\n",
    "            if tag in tags.keys():\n",
    "                tags[tag] += 1\n",
    "            else:\n",
    "                tags[tag] = 1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = getFileContents('data/en_train_tagged.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_dict = getUniqueTags(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpenProbabilities(tagged_data):\n",
    "    sentences_count = len(tagged_data)\n",
    "    open_tag_count_dict = {}\n",
    "    for line in tagged_data:\n",
    "        first_word_tag_pairs = line.strip().split(' ')[0]\n",
    "        word, tag = splitWordTag(first_word_tag_pairs)\n",
    "        if tag == 'XX':\n",
    "            print line\n",
    "        \n",
    "        if tag in open_tag_count_dict.keys():\n",
    "            open_tag_count_dict[tag] += 1\n",
    "        else:\n",
    "            open_tag_count_dict[tag] = 1\n",
    "    open_tag_count_dict.update((tag, (occurances*1.0)/sentences_count) for tag, occurances in open_tag_count_dict.items())\n",
    "    return open_tag_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCloseProbabilities(tagged_data):\n",
    "    sentences_count = len(tagged_data)\n",
    "    close_tag_count_dict = {}\n",
    "    for line in tagged_data:\n",
    "        last_word_tag_pairs = line.strip().split(' ')[-1]\n",
    "        word, tag = splitWordTag(last_word_tag_pairs)\n",
    "        if tag == 'XX':\n",
    "            print line\n",
    "        \n",
    "        if tag in close_tag_count_dict.keys():\n",
    "            close_tag_count_dict[tag] += 1\n",
    "        else:\n",
    "            close_tag_count_dict[tag] = 1\n",
    "    close_tag_count_dict.update((tag, (occurances*1.0)/sentences_count) for tag, occurances in close_tag_count_dict.items())\n",
    "    return close_tag_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%/XX\n",
      "\n",
      "%/XX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opening_probabilities = getOpenProbabilities(tagged_data)\n",
    "closing_probabilities = getCloseProbabilities(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0.004783544606553456,\n",
       " ',': 0.0009567089213106913,\n",
       " '-LRB-': 0.010922426851630392,\n",
       " '.': 0.0005580802040979033,\n",
       " 'ADD': 0.008690106035238778,\n",
       " 'CC': 0.023120465598341706,\n",
       " 'CD': 0.03236865183767838,\n",
       " 'DT': 0.11249302399744877,\n",
       " 'EX': 0.009009009009009009,\n",
       " 'FW': 0.00023917723032767282,\n",
       " 'GW': 0.00366738419835765,\n",
       " 'IN': 0.07813122857370645,\n",
       " 'JJ': 0.03332536075898908,\n",
       " 'JJR': 0.000797257434425576,\n",
       " 'JJS': 0.004624093119668341,\n",
       " 'LS': 0.006696962449174839,\n",
       " 'MD': 0.010045443673762257,\n",
       " 'NFP': 0.013075021924579447,\n",
       " 'NN': 0.051024475803236866,\n",
       " 'NNP': 0.11600095670892131,\n",
       " 'NNPS': 0.0008769831778681336,\n",
       " 'NNS': 0.015387068484413617,\n",
       " 'PDT': 0.0011161604081958065,\n",
       " 'PRP': 0.22458741927768477,\n",
       " 'PRP$': 0.02064896755162242,\n",
       " 'RB': 0.06569401259666746,\n",
       " 'RBR': 0.0007175316909830184,\n",
       " 'RBS': 0.0001594514868851152,\n",
       " 'SYM': 7.97257434425576e-05,\n",
       " 'TO': 0.0017539663557362673,\n",
       " 'UH': 0.03173084589013792,\n",
       " 'VB': 0.035398230088495575,\n",
       " 'VBD': 0.0058997050147492625,\n",
       " 'VBG': 0.008371203061468548,\n",
       " 'VBN': 0.005580802040979032,\n",
       " 'VBP': 0.016024874431954078,\n",
       " 'VBZ': 0.008132025831140876,\n",
       " 'WDT': 0.002551223790161843,\n",
       " 'WP': 0.008211751574583433,\n",
       " 'WRB': 0.0146695367934306,\n",
       " 'XX': 7.97257434425576e-05,\n",
       " '``': 0.011799410029498525}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opening_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\": 0.013154747668022004,\n",
       " ',': 0.023040739854899146,\n",
       " '-RRB-': 0.015147891254085945,\n",
       " '.': 0.7904807462329586,\n",
       " ':': 0.013553376385234793,\n",
       " 'ADD': 0.016503228892609422,\n",
       " 'CC': 0.00023917723032767282,\n",
       " 'CD': 0.007494219883600415,\n",
       " 'DT': 7.97257434425576e-05,\n",
       " 'JJ': 0.003428206968029977,\n",
       " 'JJR': 0.00023917723032767282,\n",
       " 'JJS': 0.0003189029737702304,\n",
       " 'LS': 0.0003189029737702304,\n",
       " 'MD': 7.97257434425576e-05,\n",
       " 'NFP': 0.01132105556884318,\n",
       " 'NN': 0.038268356852427646,\n",
       " 'NNP': 0.04903133221717292,\n",
       " 'NNPS': 0.0015147891254085944,\n",
       " 'NNS': 0.006776688192617396,\n",
       " 'POS': 0.0001594514868851152,\n",
       " 'PRP': 0.0008769831778681336,\n",
       " 'RB': 0.0026309495336044007,\n",
       " 'RP': 7.97257434425576e-05,\n",
       " 'SYM': 0.000398628717212788,\n",
       " 'UH': 0.0013553376385234792,\n",
       " 'VB': 0.0011161604081958065,\n",
       " 'VBG': 0.0001594514868851152,\n",
       " 'VBN': 0.0012756118950809216,\n",
       " 'VBP': 0.000398628717212788,\n",
       " 'VBZ': 0.0003189029737702304,\n",
       " 'WRB': 7.97257434425576e-05,\n",
       " 'XX': 7.97257434425576e-05,\n",
       " '``': 7.97257434425576e-05}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closing_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print len(opening_probabilities.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTransitionMatrix(tagged_data, tags_dict):\n",
    "    tags = tags_dict.keys()\n",
    "    tags.sort()\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    for index, tag in enumerate(tags):\n",
    "        tags_index_dict[tag] = index\n",
    "    \n",
    "    tag_count = len(tags)\n",
    "    transition_matrix = np.zeros(shape=(tag_count, tag_count))\n",
    "    \n",
    "    for line in tagged_data:\n",
    "        prev_tag = None\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            \n",
    "            if prev_tag is not None:\n",
    "                transition_matrix[tags_index_dict[prev_tag]][tags_index_dict[tag]] += 1\n",
    "            \n",
    "            prev_tag = tag\n",
    "            \n",
    "    probability_transition_matrix = transition_matrix/transition_matrix.sum(axis=1, keepdims=True)\n",
    "    probability_transition_matrix[np.isnan(probability_transition_matrix)] = 0\n",
    "    return probability_transition_matrix, tags_index_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "probability_transition_matrix, tags_index_dict = buildTransitionMatrix(tagged_data, tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(tagged_data):\n",
    "    words = []\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            words.append(word)\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEmissionProbabilities(tagged_data, tags_dict):\n",
    "    tags = tags_dict.keys()\n",
    "    tags.sort()\n",
    "    \n",
    "    words = getUniqueWords(tagged_data)\n",
    "    words.sort()\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    for index, tag in enumerate(tags):\n",
    "        tags_index_dict[tag] = index\n",
    "        \n",
    "    words_index_dict = {}\n",
    "    for index, word in enumerate(words):\n",
    "        words_index_dict[word] = index\n",
    "    \n",
    "    tag_count = len(tags)\n",
    "    word_count = len(words)\n",
    "    \n",
    "    emission_matrix = np.zeros(shape=(tag_count, word_count))\n",
    "    \n",
    "    for line in tagged_data:\n",
    "        prev_tag = None\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            \n",
    "            emission_matrix[tags_index_dict[tag]][words_index_dict[word]] += 1\n",
    "            \n",
    "            prev_tag = tag\n",
    "            \n",
    "    print emission_matrix[tags_index_dict['VBD'], words_index_dict['asked']]\n",
    "            \n",
    "    probability_emission_matrix = emission_matrix/emission_matrix.sum(axis=1, keepdims=True)\n",
    "    probability_emission_matrix[np.isnan(probability_emission_matrix)] = 0\n",
    "    print probability_emission_matrix\n",
    "    return probability_emission_matrix, tags_index_dict, words_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.0\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.03694268  0.          0.        ]\n",
      " [ 0.          0.          0.00012412 ...,  0.          0.          0.00012412]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.00123001  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "probability_emission_matrix, tags_index_dict, words_index_dict = computeEmissionProbabilities(tagged_data, tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEmissionProbabilities():\n",
    "    counter = 0\n",
    "    global probability_emission_matrix, tags_index_dict, words_index_dict\n",
    "    word_count = len(words_index_dict.keys())\n",
    "    tag_count = len(tags_index_dict.keys())\n",
    "    for word, word_index in words_index_dict.iteritems():\n",
    "        for tag, tag_index in tags_index_dict.iteritems():\n",
    "            if probability_emission_matrix[tag_index][word_index] != 0:\n",
    "                print tag, \" => \", word, ' => ', probability_emission_matrix[tag_index][word_index]\n",
    "                counter += 1\n",
    "                if counter > 20:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN  =>  Unemployent  =>  3.71581450654e-05\n",
      "NNS  =>  Andreas  =>  0.000118553645525\n",
      "NN  =>  rebel  =>  7.43162901308e-05\n",
      "NNP  =>  CONFIRMIT  =>  8.03277371676e-05\n",
      "MD  =>  Woud  =>  0.000303582270795\n",
      "JJ  =>  unpretentious  =>  8.62663906142e-05\n",
      "ADD  =>  http://www.tecsoc.org/pubs/history/2002/apr26.htm  =>  0.00342465753425\n",
      "JJ  =>  yellow  =>  0.000517598343685\n",
      "NNPS  =>  Heights  =>  0.00200803212851\n",
      "CD  =>  four  =>  0.00800800800801\n",
      "VBZ  =>  Does  =>  0.00349497597204\n",
      "VBG  =>  hanging  =>  0.0012012012012\n",
      "NN  =>  hanging  =>  3.71581450654e-05\n",
      "NN  =>  flatbread  =>  7.43162901308e-05\n",
      "JJ  =>  succulent  =>  8.62663906142e-05\n",
      "IN  =>  Until  =>  0.00019303156066\n",
      "NN  =>  aggression  =>  3.71581450654e-05\n",
      "NN  =>  payoff  =>  3.71581450654e-05\n",
      "VBG  =>  looking  =>  0.0246246246246\n",
      "JJ  =>  LAST  =>  8.62663906142e-05\n",
      "VBG  =>  granting  =>  0.0003003003003\n"
     ]
    }
   ],
   "source": [
    "printEmissionProbabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostProbableTags(sentence):\n",
    "    global opening_probabilities, closing_probabilities, probability_transition_matrix, probability_emission_matrix, tags_index_dict, words_index_dict\n",
    "    \n",
    "    sentence_words = sentence.strip().split(' ')\n",
    "    \n",
    "    sentence_len = len(sentence_words)\n",
    "    tag_count = len(tags_index_dict.keys())\n",
    "    \n",
    "    viterbi_matrix = np.zeros(shape=(tag_count, sentence_len))\n",
    "    \n",
    "    tracing_matrix = [[None for x in range(sentence_len)] for y in range(tag_count)]\n",
    "    \n",
    "    for word_index in range(sentence_len):\n",
    "        word = sentence_words[word_index]\n",
    "        for model_tag, model_tag_index in tags_index_dict.iteritems():\n",
    "            \n",
    "            word_emission_probability = 0.0\n",
    "            if word in words_index_dict.keys():\n",
    "                word_emission_probability = probability_emission_matrix[model_tag_index][words_index_dict[word]]\n",
    "            \n",
    "            if word_index == 0:\n",
    "                if model_tag in opening_probabilities.keys():\n",
    "                    viterbi_matrix[model_tag_index][word_index] = opening_probabilities[model_tag] * word_emission_probability\n",
    "                else:\n",
    "                    viterbi_matrix[model_tag_index][word_index] = 0.0\n",
    "            else:\n",
    "                max_probability = -1\n",
    "                max_tag = None\n",
    "                for prev_model_tag, prev_model_tag_index in tags_index_dict.iteritems():\n",
    "                    \n",
    "                    temp_probability = viterbi_matrix[prev_model_tag_index][word_index-1] * probability_transition_matrix[prev_model_tag_index][model_tag_index] * word_emission_probability  \n",
    "                    if temp_probability > max_probability:\n",
    "                        max_probability = temp_probability\n",
    "                        max_tag = prev_model_tag\n",
    "                        \n",
    "                viterbi_matrix[model_tag_index][word_index] = max_probability\n",
    "                tracing_matrix[model_tag_index][word_index] = max_tag\n",
    "    \n",
    "    max_probability = -1\n",
    "    max_probability_tag = None\n",
    "    for model_tag, model_tag_index in tags_index_dict.iteritems():\n",
    "        temp_probability = 0.0\n",
    "        if model_tag in closing_probabilities.keys():\n",
    "            temp_probability =  closing_probabilities[model_tag] * viterbi_matrix[model_tag_index][sentence_len-1]\n",
    "        if temp_probability > max_probability:\n",
    "            max_probability = temp_probability\n",
    "            max_probability_tag = model_tag\n",
    "\n",
    "    assigned_tags = [max_probability_tag]\n",
    "    current_best_tag = max_probability_tag\n",
    "    for col in range(sentence_len-1, 0, -1):\n",
    "        current_best_tag = tracing_matrix[tags_index_dict[current_best_tag]][col]\n",
    "        assigned_tags.append(current_best_tag)\n",
    "    assigned_tags = assigned_tags[::-1]\n",
    "    \n",
    "    anotated_sentence = ''\n",
    "    for index, assigned_tag in enumerate(assigned_tags):\n",
    "        anotated_sentence += str(sentence_words[index]) + '/' + str(assigned_tag) + ' '\n",
    "        \n",
    "    print anotated_sentence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One/CD of/IN them/PRP was/VBD from/IN the/DT Jubur/NNP tribe/NN and/CC was/VBD deputy/NN commander/NN of/IN the/DT Hawijah/NNP garrison/NN ./. \n"
     ]
    }
   ],
   "source": [
    "sentence = \"One of them was from the Jubur tribe and was deputy commander of the Hawijah garrison .\"\n",
    "getMostProbableTags(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One/CD of/IN them/PRP was/VBD from/IN the/DT Jubur/NNP tribe/NN and/CC was/VBD deputy/NN commander/NN of/IN the/DT Hawijah/NNP garrison/NN ./.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"One/CD of/IN them/PRP was/VBD from/IN the/DT Jubur/NNP tribe/NN and/CC was/VBD deputy/NN commander/NN of/IN the/DT Hawijah/NNP garrison/NN ./.\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
